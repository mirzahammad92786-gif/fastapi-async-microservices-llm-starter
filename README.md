# FastAPI + AsyncIO Microservices LLM Starter ğŸš€

A production-grade backend starter template built with **FastAPI**, **AsyncIO**, **PostgreSQL**, **Celery**, **Docker**, and optional **OpenAI/LLM integration**.  
Designed for scalable, high-performance systems with clean architecture and real-world patterns.

This boilerplate gives you:
- âš¡ High-speed APIs using FastAPI
- ğŸ”„ Async processing using AsyncIO
- ğŸ—ƒ PostgreSQL + SQLAlchemy for reliable data storage
- ğŸ§µ Background processing with Celery + Redis
- ğŸ” JWT authentication and role-based access
- ğŸ¤– Optional OpenAI LLM workflows (embeddings, function-calling)
- ğŸ³ Full Docker Compose setup
- ğŸ§± Clean, production-ready folder structure

---

## ğŸš€ Features

### **1. FastAPI Microservices Architecture**
- Separate modules for auth, users, tasks, AI, and integrations  
- Dependency injection and router-level modularity  
- Automatic validation with Pydantic models  

### **2. Async-Powered Backend**
- Fully async FastAPI routes  
- Async SQL queries  
- Async external API calls  
- Async background pipelines  

### **3. PostgreSQL + SQLAlchemy**
- Models, migrations, and database session management  
- Query optimization patterns  
- Indexing and schema best practices  

### **4. Celery Task Queues**
- Background tasks for emails, analytics, data processing  
- Retries + structured logging  
- Async pipelines integrated with Redis/Broker  

### **5. Authentication**
- Secure JWT token system  
- Login, refresh tokens, password hashing  
- Role-based access control  

### **6. LLM / OpenAI Integration (Optional)**
- Function-calling API wrapper  
- Embeddings-based search using FAISS/Chroma  
- Prompt-based automation utilities  

---

## ğŸ§± Project Structure

fastapi-app/
â”‚â”€â”€ app/
â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â”œâ”€â”€ auth/
â”‚ â”‚ â”œâ”€â”€ users/
â”‚ â”‚ â”œâ”€â”€ tasks/
â”‚ â”‚ â””â”€â”€ ai/
â”‚ â”œâ”€â”€ core/
â”‚ â”‚ â”œâ”€â”€ config.py
â”‚ â”‚ â”œâ”€â”€ security.py
â”‚ â”‚ â””â”€â”€ logging.py
â”‚ â”œâ”€â”€ db/
â”‚ â”‚ â”œâ”€â”€ session.py
â”‚ â”‚ â”œâ”€â”€ models.py
â”‚ â”‚ â””â”€â”€ migrations/
â”‚ â”œâ”€â”€ services/
â”‚ â”œâ”€â”€ utils/
â”‚ â””â”€â”€ main.py
â”‚â”€â”€ celery_worker/
â”‚â”€â”€ docker-compose.yml
â”‚â”€â”€ requirements.txt

yaml
Copy code

---

## ğŸ³ Docker Setup

docker-compose up --build

yaml
Copy code

This launches:
- FastAPI server  
- PostgreSQL  
- Redis  
- Celery worker  
- Celery beat (scheduled tasks)  

---

## â–¶ï¸ Running Locally

### Install dependencies
pip install -r requirements.txt

shell
Copy code

### Run API server
uvicorn app.main:app --reload

yaml
Copy code

---

## ğŸ”¬ LLM Integration Example (OpenAI)

```python
from openai import OpenAI

client = OpenAI(api_key="YOUR_KEY")

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Summarize this text"}]
)

print(response.choices[0].message["content"])
